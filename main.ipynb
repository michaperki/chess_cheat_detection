{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc8f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chess in c:\\users\\perkd\\anaconda3\\lib\\site-packages (1.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f64c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33205a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a1f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pgn_game(game):\n",
    "    # Extract relevant metadata\n",
    "    metadata = {\n",
    "        \"Event\": game.headers.get('Event', \"\"),\n",
    "        \"Site\": game.headers.get('Site', \"\"),\n",
    "        \"Date\": game.headers.get('Date', \"\"),\n",
    "        \"White\": game.headers.get('White', \"\"),\n",
    "        \"Black\": game.headers.get('Black', \"\"),\n",
    "        \"Result\": game.headers.get('Result', \"\"),\n",
    "        \"WhiteElo\": game.headers.get('WhiteElo', 0),\n",
    "        \"BlackElo\": game.headers.get('BlackElo', 0),\n",
    "        \"TimeControl\": game.headers.get('TimeControl', \"\"),\n",
    "        \"Termination\": game.headers.get('Termination', \"\")\n",
    "    }\n",
    "\n",
    "    # Check if any essential metadata is missing\n",
    "    if \"?\" in [metadata[\"White\"], metadata[\"Black\"]] or \"?\" in str(metadata[\"Date\"]) or \"????.\" in str(metadata[\"Date\"]):\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Initialize variables to store first clock times for white and black\n",
    "    white_first_clock = None\n",
    "    black_first_clock = None\n",
    "\n",
    "    # Extract moves and evaluations\n",
    "    moves = []\n",
    "    clocks = []\n",
    "    evaluations = []\n",
    "    for node in game.mainline():\n",
    "        move = node.move\n",
    "        moves.append(str(move))\n",
    "        move_data = node.comment\n",
    "        if move_data:\n",
    "            move_data_parts = move_data.split(' ')\n",
    "            if len(move_data_parts) >= 4:  # Check if move_data has enough elements\n",
    "                clock = move_data_parts[3].strip(']')\n",
    "                evaluation = move_data_parts[1].strip(']')\n",
    "                # Handle mate scores\n",
    "                if evaluation.startswith('#'):\n",
    "                    evaluation = np.sign(float(evaluation.replace('#', ''))) * float('inf')\n",
    "                else:\n",
    "                    evaluation = float(evaluation)\n",
    "            else:\n",
    "                evaluation = None\n",
    "                clock = None\n",
    "        else:\n",
    "            evaluation = None\n",
    "            clock = None\n",
    "        evaluations.append(evaluation)\n",
    "        clocks.append(clock)\n",
    "        \n",
    "        # Check and store the clock times for the first move of each player\n",
    "        if white_first_clock is None and node.board().turn:\n",
    "            white_first_clock = clock\n",
    "        elif black_first_clock is None and not node.board().turn:\n",
    "            black_first_clock = clock\n",
    "\n",
    "            # If the first clock times of white and black are not the same, exclude this game\n",
    "    if white_first_clock != black_first_clock:\n",
    "        return None, None, None, None\n",
    "\n",
    "    return metadata, moves, clocks, evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359c5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io  # Import io module\n",
    "\n",
    "def preprocess_pgn(pgn_file):\n",
    "    game_data = []\n",
    "    current_game_lines = []\n",
    "\n",
    "    with open(pgn_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"[Event\"):\n",
    "                if current_game_lines:\n",
    "                    game = \"\\n\".join(current_game_lines)\n",
    "                    parsed_game = chess.pgn.read_game(io.StringIO(game))\n",
    "                    if parsed_game:\n",
    "                        metadata, moves, clocks, evaluations = parse_pgn_game(parsed_game)\n",
    "                        if metadata is not None:\n",
    "                            game_data.append((metadata, moves, clocks, evaluations))\n",
    "                current_game_lines = []  # Reset for the next game\n",
    "            current_game_lines.append(line)\n",
    "\n",
    "        # Process the last game if it exists\n",
    "        if current_game_lines:\n",
    "            game = \"\\n\".join(current_game_lines)\n",
    "            parsed_game = chess.pgn.read_game(io.StringIO(game))\n",
    "            if parsed_game:\n",
    "                metadata, moves, clocks, evaluations = parse_pgn_game(parsed_game)\n",
    "                if metadata is not None:\n",
    "                    game_data.append((metadata, moves, clocks, evaluations))\n",
    "\n",
    "    return game_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4385baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_features_dataframe(game_data):\n",
    "    dfs = []\n",
    "    for game_id, (metadata, moves, clocks, evaluations) in enumerate(game_data):\n",
    "        if metadata is None:\n",
    "            continue  # Skip this game if metadata is missing\n",
    "\n",
    "        # Extract metadata features\n",
    "        metadata_features = [\n",
    "            game_id,  # This is the unique identifier for each game\n",
    "            metadata[\"WhiteElo\"],\n",
    "            metadata[\"BlackElo\"],\n",
    "            metadata[\"TimeControl\"],\n",
    "            metadata[\"Result\"]\n",
    "        ]\n",
    "\n",
    "        # Extract move features (e.g., move count)\n",
    "        move_count = len(moves)\n",
    "\n",
    "        # Combine all features\n",
    "        game_features = metadata_features + [move_count]\n",
    "\n",
    "        # Create DataFrame for current game\n",
    "        game_df = pd.DataFrame(columns=[\n",
    "            \"GameID\", \"WhiteElo\", \"BlackElo\", \"TimeControl\", \"Result\", \"MoveCount\", \"Move\", \"Turn\", \"Evaluation\", \"Clock\"\n",
    "        ])\n",
    "\n",
    "        # Append move, turn, and evaluation for each move separately\n",
    "        for i, move in enumerate(moves):\n",
    "            move_features = game_features + [\n",
    "                move, \n",
    "                'White' if i % 2 == 0 else 'Black', \n",
    "                evaluations[i] if evaluations else None, \n",
    "                clocks[i]\n",
    "            ]  # Add move, turn, and evaluation for the current move\n",
    "            game_df.loc[i] = move_features\n",
    "\n",
    "        dfs.append(game_df)\n",
    "\n",
    "    # Concatenate DataFrames for all games\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc147269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the data\n",
    "def preprocess_data(df):\n",
    "    # Replace NaNs and infinities\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.replace([np.inf, -np.inf], [100, -100], inplace=True)\n",
    "    \n",
    "    # Convert ELO ratings to numeric and drop rows with missing ratings\n",
    "    df['WhiteElo'] = pd.to_numeric(df['WhiteElo'], errors='coerce')\n",
    "    df['BlackElo'] = pd.to_numeric(df['BlackElo'], errors='coerce')\n",
    "    df.dropna(subset=['WhiteElo', 'BlackElo'], inplace=True)\n",
    "\n",
    "    # Calculate time-related features\n",
    "    df['Clock'] = pd.to_timedelta(df['Clock'])\n",
    "    if 'GameID' in df.columns:\n",
    "        df['TimeSpent'] = df.groupby(['GameID'])['Clock'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    # Extract time control data\n",
    "    time_control_df = df['TimeControl'].str.extract(r'(?P<InitialTime>\\d+)(?:\\+(?P<Increment>\\d+))?')\n",
    "    time_control_df = time_control_df.fillna(0).astype(float)\n",
    "    df['InitialTime'] = time_control_df['InitialTime']\n",
    "    df['Increment'] = time_control_df['Increment']\n",
    "\n",
    "    # Add time pressure feature\n",
    "    time_pressure_threshold = 10  # Define the time pressure threshold\n",
    "    df['TimePressure'] = df['Clock'].dt.total_seconds().lt(time_pressure_threshold).astype(int)\n",
    "\n",
    "    # Calculate the absolute rating difference\n",
    "    df[\"RatingDifference\"] = abs(df[\"WhiteElo\"] - df[\"BlackElo\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "945395af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/evals.pgn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pgn_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/evals.pgn\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to your PGN file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m game_data \u001b[38;5;241m=\u001b[39m preprocess_pgn(pgn_file_path)\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m create_features_dataframe(game_data)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print the first few rows of the DataFrame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mpreprocess_pgn\u001b[1;34m(pgn_file)\u001b[0m\n\u001b[0;32m      4\u001b[0m game_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m current_game_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pgn_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/evals.pgn'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "pgn_file_path = \"data/evals.pgn\"  # Replace with the path to your PGN file\n",
    "game_data = preprocess_pgn(pgn_file_path)\n",
    "df = create_features_dataframe(game_data)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "len(df)\n",
    "\n",
    "df = preprocess_data(df)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da593596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to update the features DataFrame after you've added the new columns\n",
    "features = df.drop(columns=[\"WhiteElo\", \"BlackElo\", \"RatingDifference\", \"TimeControl\", \"Clock\"])  # Assuming you want to exclude TimeControl and Clock from the features\n",
    "target = df[\"RatingDifference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f907ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e217d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define preprocessing steps for numerical and categorical features\n",
    "numeric_features = features.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = [\"Turn\"]  # Update with your categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define a new RandomForestRegressor with hyperparameters that limit complexity\n",
    "model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
    "\n",
    "# If you want to fine-tune hyperparameters further, consider using GridSearchCV or RandomizedSearchCV\n",
    "parameters = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [5, 10, 15],\n",
    "    # Add more parameters here\n",
    "}\n",
    "\n",
    "# Create a new pipeline with the simplified model\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "# Define the cross-validation iterator\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# The best model found by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Model Performance:\\nMSE: {mse}\\nMAE: {mae}\\nR2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc164b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09580f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test.values.min(), y_pred.min()), max(y_test.values.max(), y_pred.max())], \n",
    "         [min(y_test.values.min(), y_pred.min()), max(y_test.values.max(), y_pred.max())], \n",
    "         color='red', linestyle='--')\n",
    "plt.title('Actual vs. Predicted Absolute Rating Differences')\n",
    "plt.xlabel('Actual Rating Differences')\n",
    "plt.ylabel('Predicted Rating Differences')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa733452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "pgn_file_path = \"data/evals_bg.pgn\"  # Replace with the path to your PGN file\n",
    "test_data = preprocess_pgn(pgn_file_path)\n",
    "test_df = create_features_dataframe(test_data)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(test_df.head())\n",
    "\n",
    "len(test_df)\n",
    "\n",
    "new_data = preprocess_data(test_df)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features dataset and make predictions\n",
    "new_features = new_data.drop(columns=[\"WhiteElo\", \"BlackElo\", \"RatingDifference\", \"TimeControl\", \"Clock\"])\n",
    "new_predictions = pipeline.predict(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new_predictions array now holds the predicted rating differences for the new data\n",
    "new_data['PredictedRatingDifference'] = new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming new_data contains 'PredictedRatingDifference' and 'RatingDifference'\n",
    "# Summary statistics of the predictions\n",
    "prediction_summary = new_data['PredictedRatingDifference'].describe()\n",
    "print(prediction_summary)\n",
    "\n",
    "# Plot the distribution of predicted rating differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(new_data['PredictedRatingDifference'], kde=True, bins=30)\n",
    "plt.title('Distribution of Predicted Rating Differences')\n",
    "plt.xlabel('Predicted Rating Difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# If you have actual rating differences for the new dataset, you can also visualize the predictions vs actuals\n",
    "if 'RatingDifference' in new_data.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(new_data['RatingDifference'], new_data['PredictedRatingDifference'], alpha=0.5)\n",
    "    plt.plot([new_data['RatingDifference'].min(), new_data['RatingDifference'].max()],\n",
    "             [new_data['RatingDifference'].min(), new_data['RatingDifference'].max()],\n",
    "             'k--', lw=2, color='red')  # Diagonal line for reference\n",
    "    plt.title('Actual vs. Predicted Rating Differences')\n",
    "    plt.xlabel('Actual Rating Difference')\n",
    "    plt.ylabel('Predicted Rating Difference')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and plot residuals\n",
    "    new_data['Residuals'] = new_data['RatingDifference'] - new_data['PredictedRatingDifference']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(new_data['Residuals'], kde=True, bins=30)\n",
    "    plt.title('Residuals of Predictions')\n",
    "    plt.xlabel('Residual (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'new_data' is your DataFrame and it has a 'GameID' column\n",
    "# Keep only the game-level metadata and the predicted/actual rating differences\n",
    "game_level_data = new_data.groupby('GameID').agg({\n",
    "    'PredictedRatingDifference': 'mean',  # Taking the mean predicted difference per game\n",
    "    'RatingDifference': 'first',  # Assuming actual rating difference is the same for all moves in a game\n",
    "    # Include any other metadata columns you want to keep, like:\n",
    "    'WhiteElo': 'first',\n",
    "    'BlackElo': 'first',\n",
    "    'TimeControl': 'first',\n",
    "    'Result': 'first',\n",
    "    'MoveCount': 'first',\n",
    "    \n",
    "    # ... etc.\n",
    "}).reset_index()\n",
    "\n",
    "# Now let's create the visualizations using 'game_level_data'\n",
    "\n",
    "# Plot the distribution of predicted rating differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(game_level_data['PredictedRatingDifference'], kde=True, bins=30)\n",
    "plt.title('Distribution of Predicted Rating Differences - Aggregated by Game')\n",
    "plt.xlabel('Predicted Rating Difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Visualize actual vs. predicted rating differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(game_level_data['RatingDifference'], game_level_data['PredictedRatingDifference'], alpha=0.5)\n",
    "plt.plot([0, max(game_level_data['RatingDifference'].max(), game_level_data['PredictedRatingDifference'].max())],\n",
    "         [0, max(game_level_data['RatingDifference'].max(), game_level_data['PredictedRatingDifference'].max())],\n",
    "         'r--')  # Diagonal line for reference\n",
    "plt.title('Actual vs. Predicted Rating Differences - Aggregated by Game')\n",
    "plt.xlabel('Actual Rating Difference')\n",
    "plt.ylabel('Predicted Rating Difference')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot residuals\n",
    "game_level_data['Residuals'] = game_level_data['RatingDifference'] - game_level_data['PredictedRatingDifference']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(game_level_data['Residuals'], kde=True, bins=30)\n",
    "plt.title('Residuals of Predictions - Aggregated by Game')\n",
    "plt.xlabel('Residual (Actual - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a78f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_level_data_sorted_desc = game_level_data.sort_values(by='PredictedRatingDifference', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d99eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(game_level_data_sorted_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for the minimum number of occurrences required to keep the time control group\n",
    "threshold = 250  # Or any other number that makes sense for your data\n",
    "\n",
    "# Calculate the counts for each time control group\n",
    "time_control_counts = new_data['TimeControl'].value_counts()\n",
    "\n",
    "# Filter out time controls with counts below the threshold\n",
    "filtered_time_controls = time_control_counts[time_control_counts >= threshold].index.tolist()\n",
    "\n",
    "grouped = new_data.groupby('TimeControl')\n",
    "\n",
    "# Now filter the grouped data to only include those with counts above the threshold\n",
    "filtered_groups = grouped.filter(lambda x: x['TimeControl'].iloc[0] in filtered_time_controls)\n",
    "\n",
    "\n",
    "# Recalculate performance metrics for the filtered groups\n",
    "performance_metrics_filtered = pd.DataFrame(columns=['TimeControl', 'MAE', 'MSE', 'R2', 'Count'])\n",
    "\n",
    "# Recalculate the metrics only for time controls above the threshold\n",
    "for time_control in filtered_time_controls:\n",
    "    group = filtered_groups[filtered_groups['TimeControl'] == time_control]\n",
    "    mae = mean_absolute_error(group['RatingDifference'], group['PredictedRatingDifference'])\n",
    "    mse = mean_squared_error(group['RatingDifference'], group['PredictedRatingDifference'])\n",
    "    r2 = r2_score(group['RatingDifference'], group['PredictedRatingDifference'])\n",
    "    count = time_control_counts[time_control]\n",
    "\n",
    "    performance_metrics_filtered.loc[len(performance_metrics_filtered.index)] = [time_control, mae, mse, r2, count]\n",
    "\n",
    "# Now, let's plot the MAE for each time control group\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='TimeControl', y='MAE', data=performance_metrics_filtered.sort_values(by='MAE', ascending=False))\n",
    "plt.title('Mean Absolute Error by Time Control (Filtered)')\n",
    "plt.xlabel('Time Control')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd455d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific time controls we're interested in\n",
    "specific_time_controls = ['60+0', '180+0', '300+0', '600+0', '1200+0']\n",
    "\n",
    "# Filter the dataset for the specific time controls\n",
    "specific_groups = new_data[new_data['TimeControl'].isin(specific_time_controls)]\n",
    "\n",
    "# Group by the filtered 'TimeControl' and calculate performance metrics\n",
    "grouped_specific = specific_groups.groupby('TimeControl')\n",
    "\n",
    "# Initialize a DataFrame to store the performance metrics for the specific time controls\n",
    "performance_metrics_specific = pd.DataFrame(columns=['TimeControl', 'MAE', 'MSE', 'R2'])\n",
    "\n",
    "# Iterate over each group and calculate metrics\n",
    "for name, group in grouped_specific:\n",
    "    mae = mean_absolute_error(group['RatingDifference'], group['PredictedRatingDifference'])\n",
    "    mse = mean_squared_error(group['RatingDifference'], group['PredictedRatingDifference'])\n",
    "    r2 = r2_score(group['RatingDifference'], group['PredictedRatingDifference'])\n",
    "\n",
    "    # Append the performance metrics for the current time control group\n",
    "    performance_metrics_specific.loc[len(performance_metrics_specific)] = [name, mae, mse, r2]\n",
    "\n",
    "# Ensure the 'TimeControl' column is categorical with the specified order\n",
    "performance_metrics_specific['TimeControl'] = pd.Categorical(\n",
    "    performance_metrics_specific['TimeControl'],\n",
    "    categories=specific_time_controls,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by 'TimeControl' according to the specific order we want\n",
    "performance_metrics_specific.sort_values(by='TimeControl', inplace=True)\n",
    "\n",
    "# Plotting the Mean Absolute Error for the specific time controls\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='TimeControl', y='MAE', data=performance_metrics_specific)\n",
    "plt.title('Model Performance for Specific Time Controls (MAE)')\n",
    "plt.xlabel('Time Control')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ace79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific time controls we're interested in\n",
    "specific_time_controls = ['60+0', '180+0', '300+0', '600+0', '1200+0']\n",
    "\n",
    "# Filter the dataset for the specific time controls\n",
    "specific_groups = new_data[new_data['TimeControl'].isin(specific_time_controls)]\n",
    "\n",
    "# Plotting predicted vs actual for each time control\n",
    "fig, axes = plt.subplots(nrows=len(specific_time_controls), ncols=1, figsize=(10, 6 * len(specific_time_controls)))\n",
    "\n",
    "for i, time_control in enumerate(specific_time_controls):\n",
    "    # Filter the specific group\n",
    "    group = specific_groups[specific_groups['TimeControl'] == time_control]\n",
    "\n",
    "    # Create scatter plot for each time control\n",
    "    axes[i].scatter(group['RatingDifference'], group['PredictedRatingDifference'], alpha=0.5)\n",
    "    axes[i].plot([group['RatingDifference'].min(), group['RatingDifference'].max()],\n",
    "                 [group['RatingDifference'].min(), group['RatingDifference'].max()],\n",
    "                 'r--')  # Diagonal line for reference\n",
    "    axes[i].set_title(f'Actual vs. Predicted Rating Differences for Time Control {time_control}')\n",
    "    axes[i].set_xlabel('Actual Rating Difference')\n",
    "    axes[i].set_ylabel('Predicted Rating Difference')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0d64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
